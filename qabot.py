from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from huggingface_hub import HfFolder
import torch
import gradio as gr

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Global variables for caching
cached_vectordb = None
cached_file_path = None


"""def load_hf_model():
    print("Loading Hugging Face text generation model... (this runs once)")
    t5_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
    t5_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
    t5_pipe = pipeline(
        "text2text-generation", model=t5_model, tokenizer=t5_tokenizer,
        max_new_tokens=256, temperature=0.8, do_sample=True,
        device=0 if torch.cuda.is_available() else -1
    )
    # Wrap with LangChain
    llm = HuggingFacePipeline(pipeline=t5_pipe)
    print("Model loaded successfully!")
    return llm"""


def get_llm():
    try:
        #model_id = "google/gemma-3-4b-it"
        model_id = "google/gemma-1.1-2b-it"
        #model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        #model_id = "mistralai/mistral-7b-instruct-v0.3"
        
        print(f"Loading model: {model_id}")
        print("This may take a few minutes on first run...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,

        )

        # Create HuggingFace pipeline for Gemma 3
        gemma_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=256,
            temperature=0.5,
            do_sample=False,
        )

        # Wrap with LangChain
        llm = HuggingFacePipeline(pipeline=gemma_pipeline)
        print(f"Model {model_id} loaded successfully!")
        return llm
    except Exception as e:
        print(f"Error loading model: {e}")
        raise



def document_loader(file):
    """Load PDF with error handling"""
    try:
        loader = PyPDFLoader(file.name)
        loaded_document = loader.load()
        print(f"Loaded {len(loaded_document)} pages from PDF")
        return loaded_document
    except Exception as e:
        print(f"Error loading document: {e}")
        raise

def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

def hf_embedding():
    # Using sentence-transformers embedding model
    model_name = "sentence-transformers/all-mpnet-base-v2"
    hf_embedding = HuggingFaceEmbeddings(model_name=model_name)
    return hf_embedding

def vector_store(chunks):
    hf_embedding_model = hf_embedding()
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=hf_embedding_model,
    )
    return vectordb

def retrieval_qa_chain(file, query):
    """Generate QA response with retrieval augmentation"""
    try:
        global cached_vectordb, cached_file_path
        
        if not query.strip():
            return "Please enter a valid question."
        
        # Only reload if it's a different file
        if cached_file_path != file.name or cached_vectordb is None:
            print("Processing new document...")
            data = document_loader(file)
            chunks = text_splitter(data)
            print(f"Created {len(chunks)} text chunks")
            cached_vectordb = vector_store(chunks=chunks)
            cached_file_path = file.name
        
        print("Initializing LLM...")
        llm = get_llm()
        retriever_obj = cached_vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        
        # Create prompt template for better answers
        prompt = ChatPromptTemplate.from_messages([
            ("You are a helpful assistant specialized in analyzing documents. Provide clear, concise,"
            " and accurate answers based ONLY on the information provided in the document. If the answer is not in the document, "
            "clearly state that the information is not available."
            "Document context: {context}"
            "User question: {input}"
            "Response:")
        ])


        # Create the chains
        document_chain = create_stuff_documents_chain(llm, prompt)
        qa_chain = create_retrieval_chain(retriever_obj, document_chain)
        
        # Invoke the chain
        print("Generating response...")
        response = qa_chain.invoke({"input": query})
        print(response[0]['generated_text'])
        return response
    except Exception as e:
        print(f"Error in QA chain: {e}")
        return f"An error occurred: {str(e)}"


rag_app = gr.Interface(
    fn=retrieval_qa_chain,
    inputs=[
        gr.File(label="Upload PDF Document", file_types=[".pdf"]),
        gr.Textbox(label="Enter your question here", lines=3),
    ],
    outputs=gr.Textbox(label="Answer", lines=5),
    title="ðŸ“„ RAG Document QA Bot with Gemma 3",
    description="Upload a PDF document and ask questions about its content. Powered by Gemma 3, HuggingFace Embeddings, and LangChain.",
    examples=[],
    cache_examples=False,
    theme=gr.themes.Origin(),
)

if __name__ == "__main__":
    print("Starting RAG QA Bot...")
    print("Open http://127.0.0.1:7860 in your browser")
    rag_app.launch(server_name="127.0.0.1", server_port=7860, share=False)